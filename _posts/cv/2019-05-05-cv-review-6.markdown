---
layout: post
title: "Computer Vision Semester Review - Part.6"
date: "2019-05-05 20:06:35 -0400"
categories: [courses]
tags: [cv]
mathjax: true
---

This is a personal note about Computer Vision class, CS5670, in Cornell Tech.

Covering chapters: **Harris Corner Detection**, **Feature Descriptor**, **Feature Matching**

<!--more-->

# Harris Corner Detection

## Concept

| Formula | Illustration |
| $$ E(u,v) = \sum_{(x,y) \in W}{[I(x+u,y+v)-I(x,y)]^2} $$ <br><br> We want to maximize $E(u,v)$ | ![image](https://user-images.githubusercontent.com/13166286/57202825-e24d0300-6f77-11e9-8c80-a4bb17c1a223.png) |

## Approximate SSD Error

![image](https://user-images.githubusercontent.com/13166286/57205237-2a295580-6f8b-11e9-84ae-45f824a54489.png){:.w50.ctr-img}

![image](https://user-images.githubusercontent.com/13166286/57205248-3e6d5280-6f8b-11e9-879e-9539f1195e1d.png){:.w50.ctr-img}


$$ 
\begin{align}
E(u,v) & \approx Au^2 + 2Buv + Cv^2  \\
       & \approx \begin{bmatrix} u & v \end{bmatrix} \begin{bmatrix} A & B \\ B & C \end{bmatrix} \begin{bmatrix} u \\ v \end{bmatrix}
\end{align}
$$ 

where $$H = \begin{bmatrix} A & B \\ B & C \end{bmatrix}$$ is the second moment matrix. 

## Some Eigenvalue Sorcery

**TODO: Update this section**

![image](https://user-images.githubusercontent.com/13166286/57205664-19c6aa00-6f8e-11e9-8e01-70dca7ef3ac7.png){:.w80.ctr-img}

![image](https://user-images.githubusercontent.com/13166286/57205659-13d0c900-6f8e-11e9-9df3-e9ef0e08224b.png){:.w50.ctr-img}

--- 

# Invariance and Equivariance

| Type | Definition | Example
| Invariance | image is transformed and corner locations do not change | ![image](https://user-images.githubusercontent.com/13166286/57206607-0cf98480-6f95-11e9-9fd4-52c474377f27.png) | 
| Equivariance | if we have two transformed versions of the same image, features should be detected in corresponding locations | ![image](https://user-images.githubusercontent.com/13166286/57206613-20a4eb00-6f95-11e9-94c9-f3b36f89a70e.png)

# Other Features

## Laplacian of Gaussian 

| Definition | Illustration | 3D Illustration | Example
| - | - | - |
| ![image](https://user-images.githubusercontent.com/13166286/57206645-74173900-6f95-11e9-98d0-c0fee547f0b0.png) | ![image](https://user-images.githubusercontent.com/13166286/57206648-7c6f7400-6f95-11e9-9a18-f587b7c82620.png) | ![image](https://user-images.githubusercontent.com/13166286/57206664-a88af500-6f95-11e9-8797-9e82ffe362e9.png) |  ![image](https://user-images.githubusercontent.com/13166286/57206653-91e49e00-6f95-11e9-93b4-6c51af4a2cae.png) |

---

# Feature Descriptor

## Invariance & Discriminability

| Invariance | Descriptor shouldn’t change even if image is transformed |
| Discriminability | – Descriptor should be highly unique for each point |

## Multiscale Oriented PatcheS descriptor (MOPS)

![image](https://user-images.githubusercontent.com/13166286/57206792-7e860280-6f96-11e9-80dd-0d18c0bd266e.png){:.w80.ctr-img}


## Scale Invariant Feature Transform (SIFT)

Basic idea:
* Take 16x16 square window around detected feature
* Compute edge orientation (angle of the gradient - 90 degree) for each pixel
* Throw out weak edges (threshold gradient magnitude)
* Create histogram of surviving edge orientations

![image](https://user-images.githubusercontent.com/13166286/57206808-a37a7580-6f96-11e9-9832-c8fe1c127ba8.png){:.w50.ctr-img}

---

# Feature Matching

*Given features in $I_1$, How to find best match in $I_2$?*

## Feature Distance: Ratio Distance

*Idea: Pure $L_2$ distance can give small distances for ambiguous matches*

Ratio Distance = 
$$ \large{\frac{||f_1 - f_2||}{||f_1-f'_2||}} $$

| $f_2$: best SSD match to $f_1$ in $I_2$ | $f'_2$: secnod best SSD match | 

## Evaluating Matching Results

*...good old ROC curve!*

![image](https://user-images.githubusercontent.com/13166286/57206974-e2f59180-6f97-11e9-9eb7-61bb2365809f.png){:.w80.ctr-img}







